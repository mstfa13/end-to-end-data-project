{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fb78978-c371-41a7-9ff3-36a7028a8f61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE DATABASE if not exists mostafa_db;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b5956a8-9a16-4df9-9364-8c15f2fd2e84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REDACTED]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dbe27d7-dc56-4b46-9825-6b181fda4267",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9a25614-4334-44dd-9a28-9aed624fafb8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "drop table if exists staffs_dim;\n",
    "drop table if exists store_dim;\n",
    "drop table if exists stores_dim;\n",
    "drop table if exists customers_dim;\n",
    "drop table if exists status_dim;\n",
    "drop table if exists date_dim;\n",
    "drop table if exists order_fact;\n",
    "drop table if exists products_dim;\n",
    "drop table if exists product_dim;\n",
    "drop table if exists customer_dim;\n",
    "drop table if exists staff_dim;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bac8735e-3028-4926-9900-f8321d1977d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "USE mostafa_db;\n",
    "CREATE TABLE IF NOT EXISTS customer_dim\n",
    "USING DELTA\n",
    "LOCATION \"wasbs://dlbikestoreinfomart@adlsbikestoreinterns.blob.core.windows.net/mostafa/customer_dim\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bb9120b-903f-4bf3-8125-e9d12250976d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "USE mostafa_db;\n",
    "CREATE TABLE IF NOT EXISTS date_dim\n",
    "USING DELTA\n",
    "LOCATION \"wasbs://dlbikestoreinfomart@adlsbikestoreinterns.blob.core.windows.net/mostafa/date_dim\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ab34a6d-6904-48ff-b77c-bf1d273a77a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "USE mostafa_db;\n",
    "CREATE TABLE IF NOT EXISTS order_fact\n",
    "USING DELTA\n",
    "LOCATION \"wasbs://dlbikestoreinfomart@adlsbikestoreinterns.blob.core.windows.net/mostafa/order_fact\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8132b9b0-3d16-4528-b561-404c43a70ece",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from order_fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16cbb987-7167-4864-94a0-ccf55d78f95c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "USE mostafa_db;\n",
    "CREATE TABLE IF NOT EXISTS product_dim\n",
    "USING DELTA\n",
    "LOCATION \"wasbs://dlbikestoreinfomart@adlsbikestoreinterns.blob.core.windows.net/mostafa/product_dim\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31c6ee4d-3754-4240-b85e-a48475318856",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from product_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8a44471-baf0-427c-b0fc-f18697cc3c25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "USE mostafa_db;\n",
    "CREATE TABLE staffs_dim\n",
    "USING DELTA\n",
    "LOCATION \"wasbs://dlbikestoreinfomart@adlsbikestoreinterns.blob.core.windows.net/mostafa/staffs_dim\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c139ef4b-6189-4dd1-acd0-a5420032d66e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>staff_id</th><th>first_name</th><th>last_name</th><th>email</th><th>phone</th><th>active</th><th>store_id</th><th>manager_id</th><th>time_stamp</th><th>process_id</th></tr></thead><tbody><tr><td>1</td><td>Fabiola</td><td>Jackson</td><td>fabiola.jackson@bikes.shop</td><td>(831) 555-5554</td><td>1</td><td>1</td><td>NULL</td><td>2024-08-29T19:53:49.1301191Z</td><td>779c8c5b-b3cf-416d-873f-af7c02f6c3e5</td></tr><tr><td>2</td><td>Mireya</td><td>Copeland</td><td>mireya.copeland@bikes.shop</td><td>(831) 555-5555</td><td>1</td><td>1</td><td>1</td><td>2024-08-29T19:53:49.1301191Z</td><td>779c8c5b-b3cf-416d-873f-af7c02f6c3e5</td></tr><tr><td>2</td><td>Mohamed</td><td>Ali</td><td>Mohamed.d.ali@pwc.com</td><td>(xxx) xxx-xxx</td><td>1</td><td>1</td><td>1</td><td>2024-08-29T19:53:49.1301191Z</td><td>779c8c5b-b3cf-416d-873f-af7c02f6c3e5</td></tr><tr><td>3</td><td>Genna</td><td>Serrano</td><td>genna.serrano@bikes.shop</td><td>(831) 555-5556</td><td>1</td><td>1</td><td>2</td><td>2024-08-29T19:53:49.1301191Z</td><td>779c8c5b-b3cf-416d-873f-af7c02f6c3e5</td></tr><tr><td>4</td><td>Virgie</td><td>Wiggins</td><td>virgie.wiggins@bikes.shop</td><td>(831) 555-5557</td><td>1</td><td>1</td><td>2</td><td>2024-08-29T19:53:49.1301191Z</td><td>779c8c5b-b3cf-416d-873f-af7c02f6c3e5</td></tr><tr><td>5</td><td>Jannette</td><td>David</td><td>jannette.david@bikes.shop</td><td>(516) 379-4444</td><td>1</td><td>2</td><td>1</td><td>2024-08-29T19:53:49.1301191Z</td><td>779c8c5b-b3cf-416d-873f-af7c02f6c3e5</td></tr><tr><td>6</td><td>Marcelene</td><td>Boyer</td><td>marcelene.boyer@bikes.shop</td><td>(516) 379-4445</td><td>1</td><td>2</td><td>5</td><td>2024-08-29T19:53:49.1301191Z</td><td>779c8c5b-b3cf-416d-873f-af7c02f6c3e5</td></tr><tr><td>7</td><td>Venita</td><td>Daniel</td><td>venita.daniel@bikes.shop</td><td>(516) 379-4446</td><td>1</td><td>2</td><td>5</td><td>2024-08-29T19:53:49.1301191Z</td><td>779c8c5b-b3cf-416d-873f-af7c02f6c3e5</td></tr><tr><td>11</td><td>Virgie</td><td>Wiggins</td><td>virgie.wiggins@bikes.shop</td><td>(831) 555-5557</td><td>1</td><td>1</td><td>2</td><td>2024-08-29T19:53:49.1301191Z</td><td>779c8c5b-b3cf-416d-873f-af7c02f6c3e5</td></tr><tr><td>8</td><td>Kali</td><td>Vargas</td><td>kali.vargas@bikes.bop</td><td>(972) 530-5555</td><td>1</td><td>3</td><td>1</td><td>2024-08-29T19:53:49.1301191Z</td><td>779c8c5b-b3cf-416d-873f-af7c02f6c3e5</td></tr><tr><td>9</td><td>Layla</td><td>Terrell</td><td>layla.terrell@bikes.shop</td><td>(972) 530-5556</td><td>1</td><td>3</td><td>7</td><td>2024-08-29T19:53:49.1301191Z</td><td>779c8c5b-b3cf-416d-873f-af7c02f6c3e5</td></tr><tr><td>10</td><td>Bernardine</td><td>Houston</td><td>bernardine.houston@bikes.shop</td><td>(972) 530-5557</td><td>1</td><td>3</td><td>7</td><td>2024-08-29T19:53:49.1301191Z</td><td>779c8c5b-b3cf-416d-873f-af7c02f6c3e5</td></tr><tr><td>11</td><td>Jannette</td><td>David</td><td>jannette.david@bikes.shop</td><td>(516) 379-4444</td><td>1</td><td>2</td><td>1</td><td>2024-08-29T19:53:49.1301191Z</td><td>779c8c5b-b3cf-416d-873f-af7c02f6c3e5</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "1",
         "Fabiola",
         "Jackson",
         "fabiola.jackson@bikes.shop",
         "(831) 555-5554",
         "1",
         "1",
         "NULL",
         "2024-08-29T19:53:49.1301191Z",
         "779c8c5b-b3cf-416d-873f-af7c02f6c3e5"
        ],
        [
         "2",
         "Mireya",
         "Copeland",
         "mireya.copeland@bikes.shop",
         "(831) 555-5555",
         "1",
         "1",
         "1",
         "2024-08-29T19:53:49.1301191Z",
         "779c8c5b-b3cf-416d-873f-af7c02f6c3e5"
        ],
        [
         "2",
         "Mohamed",
         "Ali",
         "Mohamed.d.ali@pwc.com",
         "(xxx) xxx-xxx",
         "1",
         "1",
         "1",
         "2024-08-29T19:53:49.1301191Z",
         "779c8c5b-b3cf-416d-873f-af7c02f6c3e5"
        ],
        [
         "3",
         "Genna",
         "Serrano",
         "genna.serrano@bikes.shop",
         "(831) 555-5556",
         "1",
         "1",
         "2",
         "2024-08-29T19:53:49.1301191Z",
         "779c8c5b-b3cf-416d-873f-af7c02f6c3e5"
        ],
        [
         "4",
         "Virgie",
         "Wiggins",
         "virgie.wiggins@bikes.shop",
         "(831) 555-5557",
         "1",
         "1",
         "2",
         "2024-08-29T19:53:49.1301191Z",
         "779c8c5b-b3cf-416d-873f-af7c02f6c3e5"
        ],
        [
         "5",
         "Jannette",
         "David",
         "jannette.david@bikes.shop",
         "(516) 379-4444",
         "1",
         "2",
         "1",
         "2024-08-29T19:53:49.1301191Z",
         "779c8c5b-b3cf-416d-873f-af7c02f6c3e5"
        ],
        [
         "6",
         "Marcelene",
         "Boyer",
         "marcelene.boyer@bikes.shop",
         "(516) 379-4445",
         "1",
         "2",
         "5",
         "2024-08-29T19:53:49.1301191Z",
         "779c8c5b-b3cf-416d-873f-af7c02f6c3e5"
        ],
        [
         "7",
         "Venita",
         "Daniel",
         "venita.daniel@bikes.shop",
         "(516) 379-4446",
         "1",
         "2",
         "5",
         "2024-08-29T19:53:49.1301191Z",
         "779c8c5b-b3cf-416d-873f-af7c02f6c3e5"
        ],
        [
         "11",
         "Virgie",
         "Wiggins",
         "virgie.wiggins@bikes.shop",
         "(831) 555-5557",
         "1",
         "1",
         "2",
         "2024-08-29T19:53:49.1301191Z",
         "779c8c5b-b3cf-416d-873f-af7c02f6c3e5"
        ],
        [
         "8",
         "Kali",
         "Vargas",
         "kali.vargas@bikes.bop",
         "(972) 530-5555",
         "1",
         "3",
         "1",
         "2024-08-29T19:53:49.1301191Z",
         "779c8c5b-b3cf-416d-873f-af7c02f6c3e5"
        ],
        [
         "9",
         "Layla",
         "Terrell",
         "layla.terrell@bikes.shop",
         "(972) 530-5556",
         "1",
         "3",
         "7",
         "2024-08-29T19:53:49.1301191Z",
         "779c8c5b-b3cf-416d-873f-af7c02f6c3e5"
        ],
        [
         "10",
         "Bernardine",
         "Houston",
         "bernardine.houston@bikes.shop",
         "(972) 530-5557",
         "1",
         "3",
         "7",
         "2024-08-29T19:53:49.1301191Z",
         "779c8c5b-b3cf-416d-873f-af7c02f6c3e5"
        ],
        [
         "11",
         "Jannette",
         "David",
         "jannette.david@bikes.shop",
         "(516) 379-4444",
         "1",
         "2",
         "1",
         "2024-08-29T19:53:49.1301191Z",
         "779c8c5b-b3cf-416d-873f-af7c02f6c3e5"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 17
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "staff_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "first_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "last_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "email",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "phone",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "active",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "store_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "manager_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "time_stamp",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "process_id",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from staffs_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "382c261d-9902-439d-a5bb-eb000db43c94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "USE mostafa_db;\n",
    "CREATE TABLE store_dim\n",
    "USING DELTA\n",
    "LOCATION \"wasbs://dlbikestoreinfomart@adlsbikestoreinterns.blob.core.windows.net/mostafa/store_dim\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ded09554-1505-4570-aa31-f243d7e1889f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>store_id</th><th>store_name</th><th>phone</th><th>email</th><th>street</th><th>city</th><th>state</th><th>zip_code</th><th>time_stamp</th><th>process_id</th></tr></thead><tbody><tr><td>1</td><td>Santa Cruz Bikes</td><td>(831) 476-4321</td><td>santacruz@bikes.shop</td><td>3700 Portola Drive</td><td>Santa Cruz</td><td>CA</td><td>95060</td><td>2024-08-29T19:54:49.7832203Z</td><td>914fbcbb-bfb6-48fd-a12f-48b89a59f9e2</td></tr><tr><td>2</td><td>Baldwin Bikes</td><td>(516) 379-8888</td><td>baldwin@bikes.shop</td><td>4200 Chestnut Lane</td><td>Baldwin</td><td>NY</td><td>11432</td><td>2024-08-29T19:54:49.7832203Z</td><td>914fbcbb-bfb6-48fd-a12f-48b89a59f9e2</td></tr><tr><td>3</td><td>Rowlett Bikes</td><td>(972) 530-5555</td><td>rowlett@bikes.shop</td><td>8000 Fairway Avenue</td><td>Rowlett</td><td>TX</td><td>75088</td><td>2024-08-29T19:54:49.7832203Z</td><td>914fbcbb-bfb6-48fd-a12f-48b89a59f9e2</td></tr><tr><td>4</td><td>Mohamed Ali</td><td>NULL</td><td>Mohamed.d.ali@pwc.com</td><td>101 Good Morning</td><td>Cairo</td><td>NULL</td><td>11835</td><td>2024-08-29T19:54:49.7832203Z</td><td>914fbcbb-bfb6-48fd-a12f-48b89a59f9e2</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "1",
         "Santa Cruz Bikes",
         "(831) 476-4321",
         "santacruz@bikes.shop",
         "3700 Portola Drive",
         "Santa Cruz",
         "CA",
         "95060",
         "2024-08-29T19:54:49.7832203Z",
         "914fbcbb-bfb6-48fd-a12f-48b89a59f9e2"
        ],
        [
         "2",
         "Baldwin Bikes",
         "(516) 379-8888",
         "baldwin@bikes.shop",
         "4200 Chestnut Lane",
         "Baldwin",
         "NY",
         "11432",
         "2024-08-29T19:54:49.7832203Z",
         "914fbcbb-bfb6-48fd-a12f-48b89a59f9e2"
        ],
        [
         "3",
         "Rowlett Bikes",
         "(972) 530-5555",
         "rowlett@bikes.shop",
         "8000 Fairway Avenue",
         "Rowlett",
         "TX",
         "75088",
         "2024-08-29T19:54:49.7832203Z",
         "914fbcbb-bfb6-48fd-a12f-48b89a59f9e2"
        ],
        [
         "4",
         "Mohamed Ali",
         "NULL",
         "Mohamed.d.ali@pwc.com",
         "101 Good Morning",
         "Cairo",
         "NULL",
         "11835",
         "2024-08-29T19:54:49.7832203Z",
         "914fbcbb-bfb6-48fd-a12f-48b89a59f9e2"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 18
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "store_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "store_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "phone",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "email",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "street",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "zip_code",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "time_stamp",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "process_id",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from store_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42e8c799-411d-4b49-96c1-39a0b6021003",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "USE mostafa_db;\n",
    "CREATE TABLE IF NOT EXISTS status_dim\n",
    "USING DELTA\n",
    "LOCATION \"wasbs://dlbikestoreinfomart@adlsbikestoreinterns.blob.core.windows.net/mostafa/status_dim\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77e4e6d2-05f1-4749-bca0-c639d86e5df7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>order_status_id</th><th>order_status_description</th></tr></thead><tbody><tr><td>2</td><td>Processing</td></tr><tr><td>4</td><td>Completed</td></tr><tr><td>3</td><td>Rejected</td></tr><tr><td>1</td><td>Pending</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2,
         "Processing"
        ],
        [
         4,
         "Completed"
        ],
        [
         3,
         "Rejected"
        ],
        [
         1,
         "Pending"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 19
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "order_status_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "order_status_description",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from status_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cd61784-b3ff-4583-ba97-25058446e910",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "USE mostafa_db;\n",
    "CREATE TABLE IF NOT EXISTS product_dim\n",
    "USING DELTA\n",
    "LOCATION \"wasbs://dlbikestoreinfomart@adlsbikestoreinterns.blob.core.windows.net/mostafa/product_dim\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fc33e92-236a-4236-87e5-237d53f5f849",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from product_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e5b0157-f629-4c9b-b680-0338c466fcfe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>database</th><th>tableName</th><th>isTemporary</th></tr></thead><tbody><tr><td>mostafa_db</td><td>customer_dim</td><td>false</td></tr><tr><td>mostafa_db</td><td>date_dim</td><td>false</td></tr><tr><td>mostafa_db</td><td>order_fact</td><td>false</td></tr><tr><td>mostafa_db</td><td>product_dim</td><td>false</td></tr><tr><td>mostafa_db</td><td>staffs_dim</td><td>false</td></tr><tr><td>mostafa_db</td><td>status_dim</td><td>false</td></tr><tr><td>mostafa_db</td><td>store_dim</td><td>false</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "mostafa_db",
         "customer_dim",
         false
        ],
        [
         "mostafa_db",
         "date_dim",
         false
        ],
        [
         "mostafa_db",
         "order_fact",
         false
        ],
        [
         "mostafa_db",
         "product_dim",
         false
        ],
        [
         "mostafa_db",
         "staffs_dim",
         false
        ],
        [
         "mostafa_db",
         "status_dim",
         false
        ],
        [
         "mostafa_db",
         "store_dim",
         false
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 21
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "database",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tableName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isTemporary",
         "type": "\"boolean\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- List tables in the database to verify the table name\n",
    "SHOW TABLES IN mostafa_db;\n",
    "\n",
    "-- Select from the table if it exists\n",
    "--SELECT * FROM mostafa_db.store_dim;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38a7a1f5-8438-42d1-be42-c4119ee77b72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS sales_datamart;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3932d403-034b-4bcb-93f2-3605ec871c52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from order_fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "421ba93b-af0e-4434-b28d-5ebfb4308729",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_inserted_rows</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 24
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_inserted_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "USE mostafa_db;\n",
    "CREATE TABLE sales_datamart AS\n",
    "SELECT \n",
    "    order_fact.order_id,\n",
    "    customer_dim.first_name,\n",
    "    customer_dim.last_name,\n",
    "    customer_dim.phone,\n",
    "    customer_dim.email,\n",
    "    customer_dim.street,\n",
    "    customer_dim.city,\n",
    "    customer_dim.state,\n",
    "    customer_dim.zip_code,\n",
    "    store_dim.store_name,\n",
    "    store_dim.phone AS store_phone,\n",
    "    store_dim.email AS store_email,\n",
    "    store_dim.street AS store_street,\n",
    "    store_dim.city AS store_city,\n",
    "    store_dim.state AS store_state,\n",
    "    store_dim.zip_code AS store_zip_code,\n",
    "    staffs_dim.first_name AS staff_first_name,\n",
    "    staffs_dim.last_name AS staff_last_name,\n",
    "    staffs_dim.email AS staff_email,\n",
    "    staffs_dim.phone AS staff_phone,\n",
    "    product_dim.product_name,\n",
    "    product_dim.category_name,\n",
    "    product_dim.brand_name,\n",
    "    product_dim.model_year,\n",
    "    product_dim.list_price,\n",
    "    order_fact.order_date_id AS ordered_date,\n",
    "    order_fact.required_date_id As required_date,\n",
    "    order_fact.shipped_date_id AS shipped_date,\n",
    "    order_fact.quantity AS order_quantity,\n",
    "    order_fact.list_price AS order_price,\n",
    "    order_fact.discount,\n",
    "    order_fact.order_status\n",
    "FROM \n",
    "    order_fact\n",
    "JOIN \n",
    "    customer_dim ON order_fact.customer_id = customer_dim.customer_id\n",
    "JOIN \n",
    "    store_dim ON order_fact.store_id = store_dim.store_id\n",
    "JOIN \n",
    "    staffs_dim ON order_fact.staff_id = staffs_dim.staff_id\n",
    "JOIN \n",
    "    product_dim ON order_fact.product_id = product_dim.product_id;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51b25e08-df55-4039-8cc4-035489cffe0c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales_datamart = spark.table(\"mostafa_db.sales_datamart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b19773b8-bb09-4ad7-a38a-779bc5a621e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.adlsbikestoreinterns.blob.core.windows.net\",\n",
    "    dbutils.secrets.get(scope=\"bikes-scope\", key=\"account-key\")\n",
    ")\n",
    "\n",
    "sales_datamart.write.mode(\"overwrite\").format(\"delta\").save(\"wasbs://datamarts@adlsbikestoreinterns.blob.core.windows.net/mostafa/sales_datamart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7f3c5c6-9140-4fc7-9b31-c2b8800ab449",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from sales_datamart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e89daad4-0cf2-43a9-bc36-4344ea9ee67b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1474648143337613>, line 6\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m spark\u001B[38;5;241m.\u001B[39mconf\u001B[38;5;241m.\u001B[39mset(\n",
       "\u001B[1;32m      2\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfs.azure.account.key.adlsbikestoreinterns.blob.core.windows.net\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m      3\u001B[0m     dbutils\u001B[38;5;241m.\u001B[39msecrets\u001B[38;5;241m.\u001B[39mget(scope\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbikes-scope\u001B[39m\u001B[38;5;124m\"\u001B[39m, key\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccount-key\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      4\u001B[0m )\n",
       "\u001B[0;32m----> 6\u001B[0m sales_datamart\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwasbs://datamarts@adlsbikestoreinterns.blob.core.windows.net/mostafa/sales_datamart\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'sales_datamart' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "NameError",
        "evalue": "name 'sales_datamart' is not defined"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'sales_datamart' is not defined"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-1474648143337613>, line 6\u001B[0m\n\u001B[1;32m      1\u001B[0m spark\u001B[38;5;241m.\u001B[39mconf\u001B[38;5;241m.\u001B[39mset(\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfs.azure.account.key.adlsbikestoreinterns.blob.core.windows.net\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      3\u001B[0m     dbutils\u001B[38;5;241m.\u001B[39msecrets\u001B[38;5;241m.\u001B[39mget(scope\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbikes-scope\u001B[39m\u001B[38;5;124m\"\u001B[39m, key\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccount-key\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m )\n\u001B[0;32m----> 6\u001B[0m sales_datamart\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwasbs://datamarts@adlsbikestoreinterns.blob.core.windows.net/mostafa/sales_datamart\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "\u001B[0;31mNameError\u001B[0m: name 'sales_datamart' is not defined"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "293edafb-a100-47ce-96e5-bb203f342d50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_inserted_rows</th></tr></thead><tbody><tr><td>12901</td><td>12901</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         12901,
         12901
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 26
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_inserted_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- Insert data from the existing table into the external table\n",
    "INSERT OVERWRITE TABLE mostafa_db.sales_datamart\n",
    "SELECT * FROM mostafa_db.sales_datamart;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b85823d7-6440-4690-81cd-bbed1283aa64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `order_fact`.`ordered_date` cannot be resolved. Did you mean one of the following? [`order_fact`.`order_date_id`, `order_fact`.`order_id`, `order_fact`.`order_status`, `order_fact`.`store_name`, `order_fact`.`day_name`]. SQLSTATE: 42703; line 27 pos 4;\n",
       "'Project [order_id#12005, first_name#12044, last_name#12045, phone#12046, email#12047, street#12048, city#12049, state#12050, zip_code#12051, store_name#12065, phone#12066 AS store_phone#11926, email#12067 AS store_email#11927, street#12068 AS store_street#11928, city#12069 AS store_city#11929, state#12070 AS store_state#11930, zip_code#12071 AS store_zip_code#11931, first_name#12075 AS staff_first_name#11932, last_name#12076 AS staff_last_name#11933, email#12077 AS staff_email#11934, phone#12078 AS staff_phone#11935, product_name#12085, category_name#12091, brand_name#12089, model_year#12086, ... 8 more fields]\n",
       "+- Join Inner, (product_id#12012 = product_id#12084)\n",
       "   :- Join Inner, (staff_id#12009 = cast(staff_id#12074 as int))\n",
       "   :  :- Join Inner, (store_id#12008 = cast(store_id#12064 as int))\n",
       "   :  :  :- Join Inner, (customer_id#12006 = cast(customer_id#12043 as int))\n",
       "   :  :  :  :- SubqueryAlias spark_catalog.mostafa_db.order_fact\n",
       "   :  :  :  :  +- Relation spark_catalog.mostafa_db.order_fact[order_id#12005,customer_id#12006,order_status#12007,store_id#12008,staff_id#12009,time_stamp#12010,process_id#12011,product_id#12012,quantity#12013,list_price#12014,discount#12015,time_stamp_11#12016,process_id_12#12017,order_date_id#12018,day#12019,week#12020,month#12021,quarter#12022,year#12023,day_name#12024,month_name#12025,required_date_id#12026,day_22#12027,week_23#12028,... 14 more fields] parquet\n",
       "   :  :  :  +- SubqueryAlias spark_catalog.mostafa_db.customer_dim\n",
       "   :  :  :     +- Relation spark_catalog.mostafa_db.customer_dim[customer_id#12043,first_name#12044,last_name#12045,phone#12046,email#12047,street#12048,city#12049,state#12050,zip_code#12051,time_stamp#12052,process_id#12053] parquet\n",
       "   :  :  +- SubqueryAlias spark_catalog.mostafa_db.store_dim\n",
       "   :  :     +- Relation spark_catalog.mostafa_db.store_dim[store_id#12064,store_name#12065,phone#12066,email#12067,street#12068,city#12069,state#12070,zip_code#12071,time_stamp#12072,process_id#12073] parquet\n",
       "   :  +- SubqueryAlias spark_catalog.mostafa_db.staffs_dim\n",
       "   :     +- Relation spark_catalog.mostafa_db.staffs_dim[staff_id#12074,first_name#12075,last_name#12076,email#12077,phone#12078,active#12079,store_id#12080,manager_id#12081,time_stamp#12082,process_id#12083] parquet\n",
       "   +- SubqueryAlias spark_catalog.mostafa_db.product_dim\n",
       "      +- Relation spark_catalog.mostafa_db.product_dim[product_id#12084,product_name#12085,model_year#12086,list_price#12087,time_stamp_4#12088,brand_name#12089,time_stamp_6#12090,category_name#12091,time_stamp_8#12092,process_id#12093] parquet\n",
       "\n",
       "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:334)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:152)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8(CheckAnalysis.scala:360)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8$adapted(CheckAnalysis.scala:345)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:287)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:345)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:345)\n",
       "\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:345)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:233)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:287)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:233)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:215)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:348)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:203)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:171)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:348)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:402)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:402)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:407)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:399)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:247)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:394)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:582)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1103)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:582)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:578)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:578)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:241)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:240)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:222)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:126)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1182)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1182)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:116)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:954)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:942)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:977)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1010)\n",
       "\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:696)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$DbClassicStrategy.executeSQLQuery(DriverLocal.scala:278)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.executeSQLSubCommand(DriverLocal.scala:368)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$executeSql$1(DriverLocal.scala:389)\n",
       "\tat scala.collection.immutable.List.map(List.scala:293)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:384)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.repl(JupyterDriverLocal.scala:975)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$29(DriverLocal.scala:1109)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:128)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$24(DriverLocal.scala:1100)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:89)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:89)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:1045)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:786)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:778)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$3(DriverWrapper.scala:818)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:72)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionTags(DriverWrapper.scala:72)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.recordOperationWithResultTags(DriverWrapper.scala:72)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:818)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:685)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:730)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$runInnerLoop$1(DriverWrapper.scala:560)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:72)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:560)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:482)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:290)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "UNRESOLVED_COLUMN.WITH_SUGGESTION",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "sqlState": "42703",
        "startIndex": 800,
        "stopIndex": 822
       },
       "stackFrames": [
        "org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `order_fact`.`ordered_date` cannot be resolved. Did you mean one of the following? [`order_fact`.`order_date_id`, `order_fact`.`order_id`, `order_fact`.`order_status`, `order_fact`.`store_name`, `order_fact`.`day_name`]. SQLSTATE: 42703; line 27 pos 4;\n'Project [order_id#12005, first_name#12044, last_name#12045, phone#12046, email#12047, street#12048, city#12049, state#12050, zip_code#12051, store_name#12065, phone#12066 AS store_phone#11926, email#12067 AS store_email#11927, street#12068 AS store_street#11928, city#12069 AS store_city#11929, state#12070 AS store_state#11930, zip_code#12071 AS store_zip_code#11931, first_name#12075 AS staff_first_name#11932, last_name#12076 AS staff_last_name#11933, email#12077 AS staff_email#11934, phone#12078 AS staff_phone#11935, product_name#12085, category_name#12091, brand_name#12089, model_year#12086, ... 8 more fields]\n+- Join Inner, (product_id#12012 = product_id#12084)\n   :- Join Inner, (staff_id#12009 = cast(staff_id#12074 as int))\n   :  :- Join Inner, (store_id#12008 = cast(store_id#12064 as int))\n   :  :  :- Join Inner, (customer_id#12006 = cast(customer_id#12043 as int))\n   :  :  :  :- SubqueryAlias spark_catalog.mostafa_db.order_fact\n   :  :  :  :  +- Relation spark_catalog.mostafa_db.order_fact[order_id#12005,customer_id#12006,order_status#12007,store_id#12008,staff_id#12009,time_stamp#12010,process_id#12011,product_id#12012,quantity#12013,list_price#12014,discount#12015,time_stamp_11#12016,process_id_12#12017,order_date_id#12018,day#12019,week#12020,month#12021,quarter#12022,year#12023,day_name#12024,month_name#12025,required_date_id#12026,day_22#12027,week_23#12028,... 14 more fields] parquet\n   :  :  :  +- SubqueryAlias spark_catalog.mostafa_db.customer_dim\n   :  :  :     +- Relation spark_catalog.mostafa_db.customer_dim[customer_id#12043,first_name#12044,last_name#12045,phone#12046,email#12047,street#12048,city#12049,state#12050,zip_code#12051,time_stamp#12052,process_id#12053] parquet\n   :  :  +- SubqueryAlias spark_catalog.mostafa_db.store_dim\n   :  :     +- Relation spark_catalog.mostafa_db.store_dim[store_id#12064,store_name#12065,phone#12066,email#12067,street#12068,city#12069,state#12070,zip_code#12071,time_stamp#12072,process_id#12073] parquet\n   :  +- SubqueryAlias spark_catalog.mostafa_db.staffs_dim\n   :     +- Relation spark_catalog.mostafa_db.staffs_dim[staff_id#12074,first_name#12075,last_name#12076,email#12077,phone#12078,active#12079,store_id#12080,manager_id#12081,time_stamp#12082,process_id#12083] parquet\n   +- SubqueryAlias spark_catalog.mostafa_db.product_dim\n      +- Relation spark_catalog.mostafa_db.product_dim[product_id#12084,product_name#12085,model_year#12086,list_price#12087,time_stamp_4#12088,brand_name#12089,time_stamp_6#12090,category_name#12091,time_stamp_8#12092,process_id#12093] parquet\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:334)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:152)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8(CheckAnalysis.scala:360)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8$adapted(CheckAnalysis.scala:345)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:287)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:345)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:345)\n\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:345)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:233)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:287)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:233)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:348)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:203)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:348)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:402)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:402)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:407)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:399)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:247)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:394)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:582)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1103)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:582)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:578)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:578)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:241)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:240)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:222)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:126)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1182)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1182)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:116)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:954)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:942)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:977)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1010)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:696)\n\tat com.databricks.backend.daemon.driver.DriverLocal$DbClassicStrategy.executeSQLQuery(DriverLocal.scala:278)\n\tat com.databricks.backend.daemon.driver.DriverLocal.executeSQLSubCommand(DriverLocal.scala:368)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$executeSql$1(DriverLocal.scala:389)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:384)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.repl(JupyterDriverLocal.scala:975)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$29(DriverLocal.scala:1109)\n\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:128)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$24(DriverLocal.scala:1100)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:89)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:89)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:1045)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:786)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:778)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$3(DriverWrapper.scala:818)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:72)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionTags(DriverWrapper.scala:72)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.recordOperationWithResultTags(DriverWrapper.scala:72)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:818)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:685)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:730)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$runInnerLoop$1(DriverWrapper.scala:560)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:72)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:560)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:482)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:290)\n\tat java.lang.Thread.run(Thread.java:750)\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # %sql\n",
    "# USE mostafa_db;\n",
    "# CREATE TABLE sales_datamart AS\n",
    "# SELECT \n",
    "\n",
    "#     f.list_price as order_list_price,\n",
    "#     pd.* EXCEPT (order_timestamp, list_price),\n",
    "#     pd.list_price as product_list_price,\n",
    "#     sd.* EXCEPT (store_id, source, order_timestamp),\n",
    "#     stfd.* EXCEPT (staff_id, store_id, source, order_timestamp),\n",
    "#     cd.* EXCEPT (customer_id, source, order_timestamp),\n",
    "#     ssd.status,\n",
    "#     dd.date as ordered_date,\n",
    "#     dd2.date as required_date,\n",
    "#     dd3.date as shipped_date,\n",
    "#     -- Calculated columns\n",
    "#     f.list_price * f.quantity AS revenue_per_product,\n",
    "#     DATEDIFF(dd3.date, dd2.date) AS delay_time,\n",
    "#     f.list_price * (1 - f.discount) AS price_after_discount,\n",
    "#     (f.list_price * (1 - f.discount )) * f.quantity AS revenue_after_discount,\n",
    "#     DATEDIFF(dd3.date, dd.date) AS days_to_ship,\n",
    "#     CASE WHEN dd3.date IS NOT NULL THEN 1 ELSE 0 END AS fulfilled_order_flag,\n",
    "#     CASE WHEN f.list_price * f.quantity > 1000 THEN 1 ELSE 0 END AS high_value_order_flag,\n",
    "#     CASE WHEN f.customer_id IN (\n",
    "#         SELECT customer_id \n",
    "#         FROM mostafa_db.order_fact \n",
    "#         GROUP BY customer_id \n",
    "#         HAVING COUNT(order_id) > 5\n",
    "#     ) THEN 1 ELSE 0 END AS frequent_customer_flag\n",
    "# FROM \n",
    "#     mostafa_db.order_fact f\n",
    "# FULL OUTER JOIN \n",
    "#     mostafa_db.product_dim pd ON f.product_id = pd.product_id\n",
    "# LEFT JOIN \n",
    "#     mostafa_db.store_dim sd ON f.store_id = sd.store_id\n",
    "# LEFT JOIN \n",
    "#     mostafa_db.staffs_dim stfd ON f.staff_id = stfd.staff_id\n",
    "# LEFT JOIN \n",
    "#     mostafa_db.customer_dim cd ON f.customer_id = cd.customer_id\n",
    "# LEFT JOIN \n",
    "#     mostafa_db.status_dim ssd ON f.order_status = ssd.order_status_id\n",
    "# LEFT JOIN\n",
    "#     mostafa_db.date_dim dd ON f.order_date_id = dd.date_id\n",
    "# LEFT JOIN\n",
    "#     mostafa_db.date_dim dd2 ON f.required_date_id = dd2.date_id \n",
    "# LEFT JOIN\n",
    "#     mostafa_db.date_dim dd3 ON f.shipped_date_id = dd3.date_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10f8b508-a8f6-450d-bb61-7a651c07506b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `order_fact`.`ordered_date` cannot be resolved. Did you mean one of the following? [`order_fact`.`order_date_id`, `order_fact`.`order_id`, `order_fact`.`order_status`, `order_fact`.`store_name`, `order_fact`.`day_name`]. SQLSTATE: 42703; line 27 pos 4;\n",
       "'Project [order_id#12005, first_name#12044, last_name#12045, phone#12046, email#12047, street#12048, city#12049, state#12050, zip_code#12051, store_name#12065, phone#12066 AS store_phone#11926, email#12067 AS store_email#11927, street#12068 AS store_street#11928, city#12069 AS store_city#11929, state#12070 AS store_state#11930, zip_code#12071 AS store_zip_code#11931, first_name#12075 AS staff_first_name#11932, last_name#12076 AS staff_last_name#11933, email#12077 AS staff_email#11934, phone#12078 AS staff_phone#11935, product_name#12085, category_name#12091, brand_name#12089, model_year#12086, ... 8 more fields]\n",
       "+- Join Inner, (product_id#12012 = product_id#12084)\n",
       "   :- Join Inner, (staff_id#12009 = cast(staff_id#12074 as int))\n",
       "   :  :- Join Inner, (store_id#12008 = cast(store_id#12064 as int))\n",
       "   :  :  :- Join Inner, (customer_id#12006 = cast(customer_id#12043 as int))\n",
       "   :  :  :  :- SubqueryAlias spark_catalog.mostafa_db.order_fact\n",
       "   :  :  :  :  +- Relation spark_catalog.mostafa_db.order_fact[order_id#12005,customer_id#12006,order_status#12007,store_id#12008,staff_id#12009,time_stamp#12010,process_id#12011,product_id#12012,quantity#12013,list_price#12014,discount#12015,time_stamp_11#12016,process_id_12#12017,order_date_id#12018,day#12019,week#12020,month#12021,quarter#12022,year#12023,day_name#12024,month_name#12025,required_date_id#12026,day_22#12027,week_23#12028,... 14 more fields] parquet\n",
       "   :  :  :  +- SubqueryAlias spark_catalog.mostafa_db.customer_dim\n",
       "   :  :  :     +- Relation spark_catalog.mostafa_db.customer_dim[customer_id#12043,first_name#12044,last_name#12045,phone#12046,email#12047,street#12048,city#12049,state#12050,zip_code#12051,time_stamp#12052,process_id#12053] parquet\n",
       "   :  :  +- SubqueryAlias spark_catalog.mostafa_db.store_dim\n",
       "   :  :     +- Relation spark_catalog.mostafa_db.store_dim[store_id#12064,store_name#12065,phone#12066,email#12067,street#12068,city#12069,state#12070,zip_code#12071,time_stamp#12072,process_id#12073] parquet\n",
       "   :  +- SubqueryAlias spark_catalog.mostafa_db.staffs_dim\n",
       "   :     +- Relation spark_catalog.mostafa_db.staffs_dim[staff_id#12074,first_name#12075,last_name#12076,email#12077,phone#12078,active#12079,store_id#12080,manager_id#12081,time_stamp#12082,process_id#12083] parquet\n",
       "   +- SubqueryAlias spark_catalog.mostafa_db.product_dim\n",
       "      +- Relation spark_catalog.mostafa_db.product_dim[product_id#12084,product_name#12085,model_year#12086,list_price#12087,time_stamp_4#12088,brand_name#12089,time_stamp_6#12090,category_name#12091,time_stamp_8#12092,process_id#12093] parquet\n",
       "\n",
       "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:334)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:152)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8(CheckAnalysis.scala:360)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8$adapted(CheckAnalysis.scala:345)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:287)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:345)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:345)\n",
       "\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:345)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:233)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:287)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:233)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:215)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:348)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:203)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:171)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:348)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:402)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:402)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:407)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:399)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:247)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:394)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:582)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1103)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:582)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:578)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:578)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:241)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:240)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:222)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:126)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1182)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1182)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:116)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:954)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:942)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:977)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1010)\n",
       "\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:696)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$DbClassicStrategy.executeSQLQuery(DriverLocal.scala:278)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.executeSQLSubCommand(DriverLocal.scala:368)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$executeSql$1(DriverLocal.scala:389)\n",
       "\tat scala.collection.immutable.List.map(List.scala:293)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:384)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.repl(JupyterDriverLocal.scala:975)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$29(DriverLocal.scala:1109)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:128)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$24(DriverLocal.scala:1100)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:89)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:89)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:1045)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:786)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:778)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$3(DriverWrapper.scala:818)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:72)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionTags(DriverWrapper.scala:72)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.recordOperationWithResultTags(DriverWrapper.scala:72)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:818)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:685)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:730)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$runInnerLoop$1(DriverWrapper.scala:560)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:72)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:560)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:482)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:290)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "UNRESOLVED_COLUMN.WITH_SUGGESTION",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "sqlState": "42703",
        "startIndex": 800,
        "stopIndex": 822
       },
       "stackFrames": [
        "org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `order_fact`.`ordered_date` cannot be resolved. Did you mean one of the following? [`order_fact`.`order_date_id`, `order_fact`.`order_id`, `order_fact`.`order_status`, `order_fact`.`store_name`, `order_fact`.`day_name`]. SQLSTATE: 42703; line 27 pos 4;\n'Project [order_id#12005, first_name#12044, last_name#12045, phone#12046, email#12047, street#12048, city#12049, state#12050, zip_code#12051, store_name#12065, phone#12066 AS store_phone#11926, email#12067 AS store_email#11927, street#12068 AS store_street#11928, city#12069 AS store_city#11929, state#12070 AS store_state#11930, zip_code#12071 AS store_zip_code#11931, first_name#12075 AS staff_first_name#11932, last_name#12076 AS staff_last_name#11933, email#12077 AS staff_email#11934, phone#12078 AS staff_phone#11935, product_name#12085, category_name#12091, brand_name#12089, model_year#12086, ... 8 more fields]\n+- Join Inner, (product_id#12012 = product_id#12084)\n   :- Join Inner, (staff_id#12009 = cast(staff_id#12074 as int))\n   :  :- Join Inner, (store_id#12008 = cast(store_id#12064 as int))\n   :  :  :- Join Inner, (customer_id#12006 = cast(customer_id#12043 as int))\n   :  :  :  :- SubqueryAlias spark_catalog.mostafa_db.order_fact\n   :  :  :  :  +- Relation spark_catalog.mostafa_db.order_fact[order_id#12005,customer_id#12006,order_status#12007,store_id#12008,staff_id#12009,time_stamp#12010,process_id#12011,product_id#12012,quantity#12013,list_price#12014,discount#12015,time_stamp_11#12016,process_id_12#12017,order_date_id#12018,day#12019,week#12020,month#12021,quarter#12022,year#12023,day_name#12024,month_name#12025,required_date_id#12026,day_22#12027,week_23#12028,... 14 more fields] parquet\n   :  :  :  +- SubqueryAlias spark_catalog.mostafa_db.customer_dim\n   :  :  :     +- Relation spark_catalog.mostafa_db.customer_dim[customer_id#12043,first_name#12044,last_name#12045,phone#12046,email#12047,street#12048,city#12049,state#12050,zip_code#12051,time_stamp#12052,process_id#12053] parquet\n   :  :  +- SubqueryAlias spark_catalog.mostafa_db.store_dim\n   :  :     +- Relation spark_catalog.mostafa_db.store_dim[store_id#12064,store_name#12065,phone#12066,email#12067,street#12068,city#12069,state#12070,zip_code#12071,time_stamp#12072,process_id#12073] parquet\n   :  +- SubqueryAlias spark_catalog.mostafa_db.staffs_dim\n   :     +- Relation spark_catalog.mostafa_db.staffs_dim[staff_id#12074,first_name#12075,last_name#12076,email#12077,phone#12078,active#12079,store_id#12080,manager_id#12081,time_stamp#12082,process_id#12083] parquet\n   +- SubqueryAlias spark_catalog.mostafa_db.product_dim\n      +- Relation spark_catalog.mostafa_db.product_dim[product_id#12084,product_name#12085,model_year#12086,list_price#12087,time_stamp_4#12088,brand_name#12089,time_stamp_6#12090,category_name#12091,time_stamp_8#12092,process_id#12093] parquet\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:334)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:152)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8(CheckAnalysis.scala:360)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8$adapted(CheckAnalysis.scala:345)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:287)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:345)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:345)\n\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:345)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:233)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:287)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:233)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:348)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:203)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:348)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:402)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:402)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:407)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:399)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:247)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:394)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:582)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1103)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:582)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:578)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:578)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:241)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:240)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:222)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:126)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1182)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1182)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:116)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:954)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:942)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:977)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1010)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:696)\n\tat com.databricks.backend.daemon.driver.DriverLocal$DbClassicStrategy.executeSQLQuery(DriverLocal.scala:278)\n\tat com.databricks.backend.daemon.driver.DriverLocal.executeSQLSubCommand(DriverLocal.scala:368)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$executeSql$1(DriverLocal.scala:389)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:384)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.repl(JupyterDriverLocal.scala:975)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$29(DriverLocal.scala:1109)\n\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:128)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$24(DriverLocal.scala:1100)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:89)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:89)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:1045)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:786)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:778)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$3(DriverWrapper.scala:818)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:72)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionTags(DriverWrapper.scala:72)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.recordOperationWithResultTags(DriverWrapper.scala:72)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:818)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:685)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:730)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$runInnerLoop$1(DriverWrapper.scala:560)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:72)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:560)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:482)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:290)\n\tat java.lang.Thread.run(Thread.java:750)\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %sql\n",
    "# USE mostafa_db;\n",
    "# CREATE TABLE sales_datamart AS\n",
    "# SELECT \n",
    "\n",
    "#     f.list_price as order_list_price,\n",
    "#     pd.list_price as product_list_price,\n",
    "#     dd.date as ordered_date,\n",
    "#     dd2.date as required_date,\n",
    "#     dd3.date as shipped_date,\n",
    "#     -- Calculated columns\n",
    "#     f.list_price * f.quantity AS revenue_per_product,\n",
    "#     DATEDIFF(dd3.date, dd2.date) AS delay_time,\n",
    "#     f.list_price * (1 - f.discount) AS price_after_discount,\n",
    "#     (f.list_price * (1 - f.discount )) * f.quantity AS revenue_after_discount,\n",
    "#     DATEDIFF(dd3.date, dd.date) AS days_to_ship,\n",
    "#     CASE WHEN dd3.date IS NOT NULL THEN 1 ELSE 0 END AS fulfilled_order_flag,\n",
    "#     CASE WHEN f.list_price * f.quantity > 1000 THEN 1 ELSE 0 END AS high_value_order_flag,\n",
    "#     CASE WHEN f.customer_id IN (\n",
    "#         SELECT customer_id \n",
    "#         FROM mostafa_db.order_fact \n",
    "#         GROUP BY customer_id \n",
    "#         HAVING COUNT(order_id) > 5\n",
    "#     ) THEN 1 ELSE 0 END AS frequent_customer_flag\n",
    "# FROM \n",
    "#     mostafa_db.order_fact f\n",
    "# FULL OUTER JOIN \n",
    "#     mostafa_db.product_dim pd ON f.product_id = pd.product_id\n",
    "# LEFT JOIN \n",
    "#     mostafa_db.store_dim sd ON f.store_id = sd.store_id\n",
    "# LEFT JOIN \n",
    "#     mostafa_db.staffs_dim stfd ON f.staff_id = stfd.staff_id\n",
    "# LEFT JOIN \n",
    "#     mostafa_db.customer_dim cd ON f.customer_id = cd.customer_id\n",
    "# LEFT JOIN \n",
    "#     mostafa_db.status_dim ssd ON f.order_status = ssd.order_status_id\n",
    "# LEFT JOIN\n",
    "#     mostafa_db.date_dim dd ON f.order_date_id = dd.date_id\n",
    "# LEFT JOIN\n",
    "#     mostafa_db.date_dim dd2 ON f.required_date_id = dd2.date_id \n",
    "# LEFT JOIN\n",
    "#     mostafa_db.date_dim dd3 ON f.shipped_date_id = dd3.date_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee68f058-20cc-4b98-b354-61ebc0f4a541",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd2845d9-1a8f-4b6e-878a-82c6ba6224ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `mostafa_db`.`sales_datamart` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
       "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
       "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 1 pos 14;\n",
       "'Project [*]\n",
       "+- 'UnresolvedRelation [mostafa_db, sales_datamart], [], false\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:90)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:260)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:233)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:287)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:286)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:286)\n",
       "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
       "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
       "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
       "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
       "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:286)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:233)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:215)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:348)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:203)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:171)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:348)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:402)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:402)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:407)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:399)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:247)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:394)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:582)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1103)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:582)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:578)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:578)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:241)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:240)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:222)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:126)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1182)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1182)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:116)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:954)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:942)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:977)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1010)\n",
       "\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:696)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$DbClassicStrategy.executeSQLQuery(DriverLocal.scala:278)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.executeSQLSubCommand(DriverLocal.scala:368)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$executeSql$1(DriverLocal.scala:389)\n",
       "\tat scala.collection.immutable.List.map(List.scala:293)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:384)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.repl(JupyterDriverLocal.scala:975)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$29(DriverLocal.scala:1109)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:128)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$24(DriverLocal.scala:1100)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:89)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:89)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:1045)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:786)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:778)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$3(DriverWrapper.scala:818)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:72)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionTags(DriverWrapper.scala:72)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.recordOperationWithResultTags(DriverWrapper.scala:72)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:818)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:685)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:730)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$runInnerLoop$1(DriverWrapper.scala:560)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:72)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:560)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:482)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:290)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `mostafa_db`.`sales_datamart` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "TABLE_OR_VIEW_NOT_FOUND",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "sqlState": "42P01",
        "startIndex": 14,
        "stopIndex": 38
       },
       "stackFrames": [
        "org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `mostafa_db`.`sales_datamart` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [mostafa_db, sales_datamart], [], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:90)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:260)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:233)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:287)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:233)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:348)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:203)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:348)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:402)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:402)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:407)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:399)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:247)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:394)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:582)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1103)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:582)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:578)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:578)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:241)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:240)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:222)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:126)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1182)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1182)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:116)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:954)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:942)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:977)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1010)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:696)\n\tat com.databricks.backend.daemon.driver.DriverLocal$DbClassicStrategy.executeSQLQuery(DriverLocal.scala:278)\n\tat com.databricks.backend.daemon.driver.DriverLocal.executeSQLSubCommand(DriverLocal.scala:368)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$executeSql$1(DriverLocal.scala:389)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:384)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.repl(JupyterDriverLocal.scala:975)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$29(DriverLocal.scala:1109)\n\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:128)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$24(DriverLocal.scala:1100)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:89)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:89)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:1045)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:786)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:778)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$3(DriverWrapper.scala:818)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:72)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionTags(DriverWrapper.scala:72)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.recordOperationWithResultTags(DriverWrapper.scala:72)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:818)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:685)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:730)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$runInnerLoop$1(DriverWrapper.scala:560)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:72)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:560)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:482)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:290)\n\tat java.lang.Thread.run(Thread.java:750)\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d922726a-e662-4be6-a45f-864ecb780811",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acbe409d-6abc-40d7-afc4-053959ee135e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `order_fact`.`ordered_date` cannot be resolved. Did you mean one of the following? [`order_fact`.`order_date_id`, `order_fact`.`order_id`, `order_fact`.`order_status`, `order_fact`.`store_name`, `order_fact`.`day_name`]. SQLSTATE: 42703; line 27 pos 4;\n",
       "'Project [order_id#12005, first_name#12044, last_name#12045, phone#12046, email#12047, street#12048, city#12049, state#12050, zip_code#12051, store_name#12065, phone#12066 AS store_phone#11926, email#12067 AS store_email#11927, street#12068 AS store_street#11928, city#12069 AS store_city#11929, state#12070 AS store_state#11930, zip_code#12071 AS store_zip_code#11931, first_name#12075 AS staff_first_name#11932, last_name#12076 AS staff_last_name#11933, email#12077 AS staff_email#11934, phone#12078 AS staff_phone#11935, product_name#12085, category_name#12091, brand_name#12089, model_year#12086, ... 8 more fields]\n",
       "+- Join Inner, (product_id#12012 = product_id#12084)\n",
       "   :- Join Inner, (staff_id#12009 = cast(staff_id#12074 as int))\n",
       "   :  :- Join Inner, (store_id#12008 = cast(store_id#12064 as int))\n",
       "   :  :  :- Join Inner, (customer_id#12006 = cast(customer_id#12043 as int))\n",
       "   :  :  :  :- SubqueryAlias spark_catalog.mostafa_db.order_fact\n",
       "   :  :  :  :  +- Relation spark_catalog.mostafa_db.order_fact[order_id#12005,customer_id#12006,order_status#12007,store_id#12008,staff_id#12009,time_stamp#12010,process_id#12011,product_id#12012,quantity#12013,list_price#12014,discount#12015,time_stamp_11#12016,process_id_12#12017,order_date_id#12018,day#12019,week#12020,month#12021,quarter#12022,year#12023,day_name#12024,month_name#12025,required_date_id#12026,day_22#12027,week_23#12028,... 14 more fields] parquet\n",
       "   :  :  :  +- SubqueryAlias spark_catalog.mostafa_db.customer_dim\n",
       "   :  :  :     +- Relation spark_catalog.mostafa_db.customer_dim[customer_id#12043,first_name#12044,last_name#12045,phone#12046,email#12047,street#12048,city#12049,state#12050,zip_code#12051,time_stamp#12052,process_id#12053] parquet\n",
       "   :  :  +- SubqueryAlias spark_catalog.mostafa_db.store_dim\n",
       "   :  :     +- Relation spark_catalog.mostafa_db.store_dim[store_id#12064,store_name#12065,phone#12066,email#12067,street#12068,city#12069,state#12070,zip_code#12071,time_stamp#12072,process_id#12073] parquet\n",
       "   :  +- SubqueryAlias spark_catalog.mostafa_db.staffs_dim\n",
       "   :     +- Relation spark_catalog.mostafa_db.staffs_dim[staff_id#12074,first_name#12075,last_name#12076,email#12077,phone#12078,active#12079,store_id#12080,manager_id#12081,time_stamp#12082,process_id#12083] parquet\n",
       "   +- SubqueryAlias spark_catalog.mostafa_db.product_dim\n",
       "      +- Relation spark_catalog.mostafa_db.product_dim[product_id#12084,product_name#12085,model_year#12086,list_price#12087,time_stamp_4#12088,brand_name#12089,time_stamp_6#12090,category_name#12091,time_stamp_8#12092,process_id#12093] parquet\n",
       "\n",
       "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:334)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:152)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8(CheckAnalysis.scala:360)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8$adapted(CheckAnalysis.scala:345)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:287)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:345)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:345)\n",
       "\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:345)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:233)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:287)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:233)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:215)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:348)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:203)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:171)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:348)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:402)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:402)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:407)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:399)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:247)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:394)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:582)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1103)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:582)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:578)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:578)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:241)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:240)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:222)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:126)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1182)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1182)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:116)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:954)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:942)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:977)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1010)\n",
       "\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:696)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$DbClassicStrategy.executeSQLQuery(DriverLocal.scala:278)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.executeSQLSubCommand(DriverLocal.scala:368)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$executeSql$1(DriverLocal.scala:389)\n",
       "\tat scala.collection.immutable.List.map(List.scala:293)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:384)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.repl(JupyterDriverLocal.scala:975)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$29(DriverLocal.scala:1109)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:128)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$24(DriverLocal.scala:1100)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:89)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:89)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:1045)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:786)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:778)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$3(DriverWrapper.scala:818)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:72)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionTags(DriverWrapper.scala:72)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.recordOperationWithResultTags(DriverWrapper.scala:72)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:818)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:685)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:730)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$runInnerLoop$1(DriverWrapper.scala:560)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:72)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:560)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:482)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:290)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "UNRESOLVED_COLUMN.WITH_SUGGESTION",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "sqlState": "42703",
        "startIndex": 800,
        "stopIndex": 822
       },
       "stackFrames": [
        "org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `order_fact`.`ordered_date` cannot be resolved. Did you mean one of the following? [`order_fact`.`order_date_id`, `order_fact`.`order_id`, `order_fact`.`order_status`, `order_fact`.`store_name`, `order_fact`.`day_name`]. SQLSTATE: 42703; line 27 pos 4;\n'Project [order_id#12005, first_name#12044, last_name#12045, phone#12046, email#12047, street#12048, city#12049, state#12050, zip_code#12051, store_name#12065, phone#12066 AS store_phone#11926, email#12067 AS store_email#11927, street#12068 AS store_street#11928, city#12069 AS store_city#11929, state#12070 AS store_state#11930, zip_code#12071 AS store_zip_code#11931, first_name#12075 AS staff_first_name#11932, last_name#12076 AS staff_last_name#11933, email#12077 AS staff_email#11934, phone#12078 AS staff_phone#11935, product_name#12085, category_name#12091, brand_name#12089, model_year#12086, ... 8 more fields]\n+- Join Inner, (product_id#12012 = product_id#12084)\n   :- Join Inner, (staff_id#12009 = cast(staff_id#12074 as int))\n   :  :- Join Inner, (store_id#12008 = cast(store_id#12064 as int))\n   :  :  :- Join Inner, (customer_id#12006 = cast(customer_id#12043 as int))\n   :  :  :  :- SubqueryAlias spark_catalog.mostafa_db.order_fact\n   :  :  :  :  +- Relation spark_catalog.mostafa_db.order_fact[order_id#12005,customer_id#12006,order_status#12007,store_id#12008,staff_id#12009,time_stamp#12010,process_id#12011,product_id#12012,quantity#12013,list_price#12014,discount#12015,time_stamp_11#12016,process_id_12#12017,order_date_id#12018,day#12019,week#12020,month#12021,quarter#12022,year#12023,day_name#12024,month_name#12025,required_date_id#12026,day_22#12027,week_23#12028,... 14 more fields] parquet\n   :  :  :  +- SubqueryAlias spark_catalog.mostafa_db.customer_dim\n   :  :  :     +- Relation spark_catalog.mostafa_db.customer_dim[customer_id#12043,first_name#12044,last_name#12045,phone#12046,email#12047,street#12048,city#12049,state#12050,zip_code#12051,time_stamp#12052,process_id#12053] parquet\n   :  :  +- SubqueryAlias spark_catalog.mostafa_db.store_dim\n   :  :     +- Relation spark_catalog.mostafa_db.store_dim[store_id#12064,store_name#12065,phone#12066,email#12067,street#12068,city#12069,state#12070,zip_code#12071,time_stamp#12072,process_id#12073] parquet\n   :  +- SubqueryAlias spark_catalog.mostafa_db.staffs_dim\n   :     +- Relation spark_catalog.mostafa_db.staffs_dim[staff_id#12074,first_name#12075,last_name#12076,email#12077,phone#12078,active#12079,store_id#12080,manager_id#12081,time_stamp#12082,process_id#12083] parquet\n   +- SubqueryAlias spark_catalog.mostafa_db.product_dim\n      +- Relation spark_catalog.mostafa_db.product_dim[product_id#12084,product_name#12085,model_year#12086,list_price#12087,time_stamp_4#12088,brand_name#12089,time_stamp_6#12090,category_name#12091,time_stamp_8#12092,process_id#12093] parquet\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:334)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:152)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8(CheckAnalysis.scala:360)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8$adapted(CheckAnalysis.scala:345)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:287)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:345)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:345)\n\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:345)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:233)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:287)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:233)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:348)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:203)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:348)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:402)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:402)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:407)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:399)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:247)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:394)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:582)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1103)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:582)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:578)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:578)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:241)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:240)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:222)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:126)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1182)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1182)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:116)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:954)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:942)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:977)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1010)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:696)\n\tat com.databricks.backend.daemon.driver.DriverLocal$DbClassicStrategy.executeSQLQuery(DriverLocal.scala:278)\n\tat com.databricks.backend.daemon.driver.DriverLocal.executeSQLSubCommand(DriverLocal.scala:368)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$executeSql$1(DriverLocal.scala:389)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:384)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.repl(JupyterDriverLocal.scala:975)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$29(DriverLocal.scala:1109)\n\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:128)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$24(DriverLocal.scala:1100)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:89)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:89)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:1045)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:786)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:778)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$3(DriverWrapper.scala:818)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:72)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionTags(DriverWrapper.scala:72)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.recordOperationWithResultTags(DriverWrapper.scala:72)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:818)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:685)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:730)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$runInnerLoop$1(DriverWrapper.scala:560)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:72)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:560)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:482)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:290)\n\tat java.lang.Thread.run(Thread.java:750)\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "catalog-creation-mostafa",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
