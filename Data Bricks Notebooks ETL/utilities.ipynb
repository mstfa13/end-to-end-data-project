{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca8ce574-8adf-491b-9a28-fc72e13e1f54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cd01390-abe6-49ab-83b2-20b55896c8ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, stddev, col, when\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import DataFrameWriter\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.functions import col, mean, stddev, when, abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e33d7ea-5d3c-40a6-88d3-41de4c6e0bc7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "metadata = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"wasbs://dlbikestorelanding@adlsbikestoreinterns.blob.core.windows.net/Mostafa_Landing/metadata/metadata_mostafa.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35e280df-8863-42a5-95d4-8c7c7ce0d9f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_unique_tables(metadata):\n",
    "    return [row['TableName'] for row in metadata.select('TableName').distinct().collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d993f47b-96c5-4a84-8ed0-2af4343cae8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##load data\n",
    "def load_source_data(spark, metadata_df):\n",
    "\n",
    "    dataframes = {}\n",
    "\n",
    "    # Extract unique folder names from the metadata DataFrame\n",
    "    folder_names = metadata_df.select(\"destination_filename\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    # Load files for each folder name\n",
    "    for folder_name in folder_names:\n",
    "        file_path = f\"wasbs://dlbikestorelanding@adlsbikestoreinterns.blob.core.windows.net/Mostafa_Landing/{folder_name}/\"\n",
    "        df = spark.read.parquet(file_path, inferSchema=True)\n",
    "        dataframes[folder_name] = df\n",
    "    for source_filename, df in dataframes.items():\n",
    "     globals()[f\"{source_filename}_df\"] = df\n",
    "    \n",
    "    return dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d61260c1-c9d1-41d6-8e45-e21bd5a7eeca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##Handling Outliers using Z-Score and IQR methods \n",
    "def handle_outliers(df, column, method, z_thresh=3.0) -> DataFrame:\n",
    "\n",
    "    if method == \"zscore\" or \"Zscore\" or \"z score\" or \"Z score\" or \"z-score\" or \"Z-score\":\n",
    "        for column in df.columns:\n",
    "            mean_value = df.select(mean(col(column))).collect()[0][0]\n",
    "            stddev_value = df.select(stddev(col(column))).collect()[0][0]\n",
    "            df = df.withColumn(\n",
    "                column,\n",
    "                when(\n",
    "                    abs((col(column) - mean_value) / stddev_value) > z_thresh,\n",
    "                    None \n",
    "                ).otherwise(col(column))\n",
    "            )\n",
    "\n",
    "    elif method == \"iqr\" or \"IQR\" or \"Iqr\":\n",
    "        for column in df.columns:\n",
    "            q1 = df.approxQuantile(column, [0.25], 0.01)[0]\n",
    "            q3 = df.approxQuantile(column, [0.75], 0.01)[0]\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            df = df.withColumn(\n",
    "                column,\n",
    "                when(\n",
    "                    (col(column) < lower_bound) | (col(column) > upper_bound),\n",
    "                    None  # Mark outliers as None (can be replaced or dropped later)\n",
    "                ).otherwise(col(column))\n",
    "            )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported method! Use 'zscore' or 'iqr'.\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f70d0f6a-13d1-487a-b9d5-80dba09a2728",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def standardize_name_case(df, column_name):\n",
    "\n",
    "    lower_count = df.filter(F.lower(F.col(column_name)) == F.col(column_name)).count()\n",
    "    upper_count = df.filter(F.upper(F.col(column_name)) == F.col(column_name)).count()\n",
    "\n",
    "    if lower_count > upper_count:\n",
    "\n",
    "        standardized_df = df.withColumn(column_name, F.lower(F.col(column_name)))\n",
    "    else:\n",
    "\n",
    "        standardized_df = df.withColumn(column_name, F.upper(F.col(column_name)))\n",
    "\n",
    "    return standardized_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eea980c5-071e-4849-8e5d-8d8c3aae0a10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def handle_nulls(df, metadata, table_name):\n",
    "    # Get column names and their nullable status from metadata\n",
    "    table_metadata = metadata.filter(F.col('table_name') == table_name).collect()\n",
    "\n",
    "    for row in table_metadata:\n",
    "        column_name = row['table_name']\n",
    "        nullable = row['can_be_nulled']\n",
    "\n",
    "        if nullable == 'Y':\n",
    "            # Fill nulls with -1\n",
    "            df = df.na.fill({column_name: 'unknown'})\n",
    "        elif nullable == 'N':\n",
    "            # Drop rows where column has nulls\n",
    "            df = df.na.drop(subset=[column_name])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96b6685d-0dde-4b36-a445-85709aa7fe79",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Handling Duplicates in a DataFrame by metadata \n",
    "def handle_duplicates(df, metadata, table_name):\n",
    "    table_metadata = metadata.filter(F.col('source_filename') == table_name).collect()\n",
    "\n",
    "    for row in table_metadata:\n",
    "        column_name = row['table_name']\n",
    "        unique = row['unique?']\n",
    "\n",
    "        if unique == 'Y':\n",
    "            # Drop duplicates, keeping the first\n",
    "            df = df.drop_duplicates(subset=[column_name])\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba14f16f-52cb-499e-9bb7-3f0c04c77149",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8c1e57b-933a-4761-878b-0eab0b8778cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#validating data types and changing them if neccesary\n",
    "def change_data_type(df: DataFrame, column: str, new_type: str) -> DataFrame:\n",
    "  return df.withColumn(column, df[column].cast(new_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64f7c0ff-a331-41c6-8e66-af76ef92b650",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_column_data_type(df, table_name, metadata_df):\n",
    "\n",
    "    column_types = dict(metadata_df.filter(F.col('destination_filename') == table_name)\n",
    "                        .select('table_column', 'data_type')\n",
    "                        .collect())\n",
    "\n",
    "    for column, expected_type in column_types.items():\n",
    "        if column in df.columns:\n",
    "            if expected_type == 'int':\n",
    "                df = df.withColumn(column, F.col(column).cast(IntegerType()))\n",
    "            elif expected_type == 'float':\n",
    "                df = df.withColumn(column, F.col(column).cast(FloatType()))\n",
    "            elif expected_type == 'string': \n",
    "                df = df.withColumn(column, F.col(column).cast(StringType()))\n",
    "            elif expected_type == 'datetime':  \n",
    "                df = df.withColumn(column, F.col(column).cast(\"timestamp\"))\n",
    "            elif expected_type == 'boolean':\n",
    "                df = df.withColumn(column, F.col(column).cast(BooleanType()))\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported data type: {expected_type} for column: {column}\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c4e5304-4b11-49a5-81d9-9579a40d2829",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "utilities",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
